# Improvements to GiveWell-esque CEAs

There is a collection of possible improvements that can be done to GiveWell
CEAs. This here is a collection of possible opportunities and projects, ranked
by usefulness as perceived by Sam Nolan.

## Uncertainty Quantification
This section is all about how uncertainty quantification can be used to improve
existing evalutions.

A lot of these projects are independent explorations of each other. Because of this,
I think it would be well suited for a small team to be hired to investigate each
of these options independently. It could learn which ones are useful and which ones
are not, and further creating higher and higher quality CEAs.

### Moral Uncertainty
Currently, Sam's [Quantification of GiveDirectly's cost-effectiveness](https://observablehq.com/@hazelfire/givewells-givedirectly-cost-effectiveness-analysis)
does not have any uncertainty about moral parameters. I believe the Happier Lives
Institute may have already done something to this end. I'll need to read their
work in more detail to understand to what extent this was done.

### Value of Information
It could be possible to calculate how much more research in a particular intervention
is worth to people in decision relevant places. For instance, say a person was
to do a study to decrease uncertainty about a parameter, how much
should you spend on that study given that the information will reduce uncertainty
about the intervention to only a certain extent.

I am quite excited about this, because I'm not aware of any attempts
to quantify the value of research in such a comprehensive way. However, I believe
Ozzie is much less excited, because value of information calculations require
understanding the exact decision being made, and sometimes the information
might be used in unexpected useful ways in the future. I however do not represent
Ozzie, so do not think this accurately represents his views.

### Forecasting integrations
Integrations could be possible with forecasting platforms such as Metaculus,
Manifold Markets or similar, importing a prediction market's uncertainty on an
issue could then make estimations a community effort, and much more grounded.

There's also integrations the other way, using functions to forecast parameters
that change over time, and scoring those changes. Ozzie is particularly excited
about this pathway.

Manifold markets has recently integrated scalar markets, which could make this
more possible.

### Wiki and Forum integrations
Another possible pathway would be to integrate estimation and forecasting into
wikis and forums. This would encourage people to edit and create their own estimations
in their forum posts. Some work could then be done to sketch out what some
initial posts might look like, and work out how these estimates could be used
by other posts and the like.

I believe Quinn was talking to the EA Forum about integrations for Squiggle. I
think the EA Forum developers seemed interested.

## Type checking and Validation
[Pedant](https://forum.effectivealtruism.org/posts/xue4yQ5rn6iDsHdmM/pedant-a-type-checker-for-cost-effectiveness-analysis) 
is a language that attempts to "Type Check" evaluations for errors in construction.
Despite my personal interest in this particular project, it's likely the least
impactful use of time on this list. I don't believe the errors caught in Pedant
style work are ever big enough to justify changing any decision.

## Hiring and Training
I believe that there is a lot of different ways that evaluations can be improved,
and a lot of evaluations that could be done. Because of this, I'm interested in
training and hiring people to fill roles that seem to be demanded by the EA
community.

### Courses, Books and Training
A Cost Effectiveness Analysis handbook would be a valuable construction. A list
of basic considerations one should make when creating a CEA, what tools can be
used, past attempts and research etc. This resource could help create a more 
standard language for which people can look to write and also improve CEAs.

An extension to a handbook would be a course, run in a similar vein to the AI
Safety Fellowship or the EA In Depth Virtual Program. This could get more people
skilled in evaluation and also scope out possible hires for other evaluation
agencies.

It could be possible to collaborate with [Training for Good](https://www.trainingforgood.com/)
for this one. I however don't know the scope of their organization. I've asked
for a call to discuss this option.

To do this we would need a bit of money to consult people in different organizations
in evaluation. Hiring people to write, construct and run the course.

### Events and Hackathons
Attending an EA Global conference talking about evaluations could be beneficial.
We could run some form of hackathon or event in evaluations to elicit other
ideas for improvement, identify talent, and progress evaluations in areas that
need progress (such as longtermist evaluations).

## Writing more evaluations
There currently just isn't enough evaluations and estimations for effectiveness.
One pathway would be to start an "evaluation consultancy". I believe QURI was
interested in going doing this path, and Charity Entrepreneurship has interest
in this for specific areas. We likely need much more people doing the type of work
that Nuno is currently doing.

This is very horizontally scalable, so could be a good fit for funding a new
org around.

### Quantifying ACE Estimations
ACE's CEAs are currently not quantified, even though they used to be. 
This could change soon, as ACE is recently looking into hiring a new executive
director which could possible include an evaluation season that is more quantitative.

The EA community could likely want to see strong quantitative evaluations for
animal charities, and helping ACE with this process could be valuable.

### Improving Shallow Evalutions of Longtermist organizations
Nuno's [Shallow Evaluation of Longtermist organizations](https://forum.effectivealtruism.org/posts/xmmqDdGqNZq5RELer/shallow-evaluations-of-longtermist-organizations) is an example
of state of the art in longtermist organizations. However, these have a very long
way to go in being useful to funders. These could be improved and detailed further
to make more comprehensive evaluations.

### Growth - Non-randomista evaluations
Evaluations are almost never done for [growth based interventions](https://forum.effectivealtruism.org/posts/bsE5t6qhGC65fEpzN/growth-and-the-case-against-randomista-development).
I've talked to Hauke about creating growth based evaluations and he believes a really
good start would be to attempt to forecast GDP growth over longer time periods.
I might be more interested in making this Hauke's case better in his post, because
it certainly hasn't gathered enough traction to have influenced the community to
any major degree that I'm aware of.

Evaluations on programs to increase GDP per capita could be very important:

"This suggests that when we are working out how to increase human welfare to the greatest extent possible, then we should start by figuring out how best to increase GDP per capita. However, to our knowledge, EAs have not publicly published any investigations of this question."

### Program Design with Charity Entepreneurship
Evaluations could go in a different direction, trying to improve the effectiveness
of new charities by use of estimation.

This could look like:
 - Having someone interested in estimation sit in on program design meetings in
   Charity Enterpreneurship.
 - Following up on Charity Entrepreneurship's [Exploratory Altruism](https://www.charityentrepreneurship.com/post/top-ea-meta-charity-ideas-for-the-2021-incubation-program)

It could start with Animal Welfare charities, that have very little information
at their disposal for past CEA work.

